{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec713e8c",
   "metadata": {},
   "source": [
    "<!-- Dense Layer -->\n",
    "A dense layer is a fully connected layer, a neural network layer where each input neuron is connected to every output neuron. It contains learnable parameters - weights and biases - that are updated during training to capture patterns in the data. Dense layers are commonly used in feedforward NNs and are often followed by activation functions to introduce non linearity.\n",
    "\n",
    "<!-- Activation Function -->\n",
    "function that introduces non linearity and make neural network able to learn curves in data instead of just linear.\n",
    "\n",
    "<!-- Forward and Backward pass -->\n",
    "Forward pass - feeding input data into the network where each layer applies a transformation z = Wx+b followed by an activation function a = f(z) continuing layer by layer until we get the final output(prediction)(inference)\n",
    "\n",
    "Backward pass - after forward pass, we compute the loss and backward pass uses backpropagation to compute the derivatives of the loss wrt to each weight and bias, done by using chain rule of calculus layer by layer in reverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231bf765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6932\n",
      "Epoch 200, Loss: 0.5338\n",
      "Epoch 400, Loss: 0.4804\n",
      "Epoch 600, Loss: 0.4786\n",
      "Epoch 800, Loss: 0.4781\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(2)\n",
    "X = np.random.randn(4, 2)\n",
    "y = np.array([[0], [1], [0], [1]])\n",
    "\n",
    "W1 = np.random.randn(2, 2) * 0.01\n",
    "b1 = np.zeros((1,2))\n",
    "W2 = np.random.randn(2, 1) * 0.01\n",
    "b2 = np.zeros((1,1))\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_deriv(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "for epoch in range(1000):\n",
    "\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    y_hat = sigmoid(z2)\n",
    "\n",
    "    m = y.shape[0]\n",
    "    loss = -np.mean(y * np.log(y_hat + 1e-8) + (1-y) * np.log(1-y_hat + 1e-8))\n",
    "\n",
    "    dz2 = y_hat - y\n",
    "    dW2 = (a1.T @ dz2) / m\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True) /m\n",
    "\n",
    "    da1 = dz2 @ W2.T\n",
    "    dz1 = da1 * relu_deriv(z1)\n",
    "    dW1 = (X.T @ dz1) / m\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True) /m \n",
    "\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr*db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr*db2\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
