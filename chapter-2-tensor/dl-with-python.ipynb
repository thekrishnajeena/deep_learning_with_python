{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e69da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630b3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((20, 100))\n",
    "y = np.random.random((20, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcfc5e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took :  0.00 s\n",
      "Shape of z:  (20, 100)\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "z = x+y\n",
    "z = np.maximum(z, 0.)\n",
    "\n",
    "print(\"Took : {0: .2f} s\".format(time.time() - t0))\n",
    "print(\"Shape of z: \", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f935e",
   "metadata": {},
   "source": [
    "Broadcasting\n",
    "\n",
    "when we try to add a high rank (let say rank 2 tensor) to a low rank(let say a vector)\n",
    ", then if there's no ambiguity, the smaller tensor will be broadcast to match of the large tensor, two steps in broadcasting: \n",
    "\n",
    "1. Axes(called broadcast axes) are added to the smaller tensor to match the ndim of the larger tensor.\n",
    "\n",
    "2. the samller tensor is repeated alongside these new axes to match the full shape of the larger tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e73b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random((32, 10))\n",
    "Y = np.random.random((10,))\n",
    "\n",
    "y = np.expand_dims(y, axis=0)\n",
    "\n",
    "Y = np.concatenate([y]*32, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db1f1d2",
   "metadata": {},
   "source": [
    "naive implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eefbd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    x = x.copy()\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82723dfb",
   "metadata": {},
   "source": [
    "Tensor product:\n",
    "\n",
    "or dot product, in numpy we do this using np.dot function \n",
    "\n",
    "x = np.random.random((32, ))\n",
    "y = np.random.random((32, ))\n",
    "z = np.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64334137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dot product of two vectors mathematically\n",
    "\n",
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) ==1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "\n",
    "    z = 0\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        z+= x[i] * y[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf238c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot product of a matrix and a vector\n",
    "\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z\n",
    "\n",
    "#reuse of two vectors dot product here\n",
    "\n",
    "def naive_matrix_vector_dot_2(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44451dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if one of the two tensors has ndim>1, dot is no longer symmetric\n",
    "#naive matrix dot \n",
    "\n",
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3798a7e",
   "metadata": {},
   "source": [
    "Tensor reshaping:\n",
    "means rearranging its rows and columns to match a target shape.\n",
    "\n",
    "Example: \n",
    "x = np.array([[0, 1],\n",
    "[2, 3],\n",
    "[4, 5]])\n",
    "\n",
    "x.shape ---> (3, 2)\n",
    "\n",
    "x = x.reshape((6,1))\n",
    "x : \n",
    "array([[0],\n",
    "[1],\n",
    "[2],\n",
    "[3],\n",
    "[4],\n",
    "[5]])\n",
    "\n",
    "\n",
    "Special case : transposition:\n",
    "x = np.zeros((300, 200))\n",
    "x = np.transpose(x)\n",
    "x.shape --> (20, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49edadff",
   "metadata": {},
   "source": [
    "elementary geometric operations such as translation, rotation, scaling, skewing and so on can be expreseed as tensor operations.\n",
    "\n",
    "Translation\n",
    "Rotation\n",
    "Scaling\n",
    "Linear transform : a dot product with an arbitrary matrix implements Linear transform, rotation and scaling are LT\n",
    "Affine transform : combination of a LT(achieved via a dot product with some matrix) and a translation(via a vector addition). \n",
    "\n",
    "A Dense layer without an activationi function is an affine layer.\n",
    "\n",
    "Dense layer with relu activation: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e3f7d",
   "metadata": {},
   "source": [
    "Gradient Tape in TensorFlow:\n",
    "\n",
    "for model we need loss function and way to calculate Y_pred,\n",
    "\n",
    "GT - keep records of all the changes we do on tensors and calculates gradient of some loss with respect to some variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
